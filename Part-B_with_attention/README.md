# Deep Learning Assignment 3: Seq2Seq Model for Transliteration with Attention

This repository contains the code for training and testing a sequence-to-sequence (Seq2Seq) model for transliteration using PyTorch. The model is trained on a dataset of Latin words and their corresponding translations in the Bangla language.

## Dataset

The [Aksharantar Dataset Ben](https://www.kaggle.com/datasets/parthasakhapaul/aksharantar) consists of Latin words paired with their Bangla translations. It is divided into training, validation, and test sets. The training set is used to train the model, the validation set is used to tune hyperparameters and monitor training progress, and the test set is used to evaluate the final model performance.

## Model Architecture

### Encoder
- **Embedding layer:** Converts input tokens into dense vectors of fixed size.
- **Bidirectional LSTM layers:** Captures bidirectional dependencies in the input sequence.
- **Dropout:** Regularization technique to prevent overfitting.
- **Context vector:** Aggregates information from the entire input sequence into a fixed-size vector.

### Decoder
- **Embedding layer:** Converts input tokens into dense vectors of fixed size.
- **LSTM layers:** Generates the output sequence based on the context vector and previous decoder states.
- **Attention mechanism:** Allows the decoder to focus on different parts of the input sequence while generating the output.

## Training

The model is trained using the Adam optimizer with a cross-entropy loss function. During training, the model learns to minimize the difference between the predicted translations and the ground truth translations in the training set.

## Hyperparameter Tuning

Hyperparameters such as embedding size, number of layers, hidden layer size, learning rate, dropout rate, and number of epochs are tuned using Bayesian optimization. Wandb is used for experiment tracking and hyperparameter search.

## Testing

After training, the model is evaluated on the test set to assess its performance on unseen data. The test accuracy and loss are reported to measure the effectiveness of the model.

## Results

The final trained model achieves an accuracy of [insert accuracy here] on the test set. Sample translations generated by the model are provided for qualitative analysis.

| Command                   | Description                                   | Accepted Values                   | Default Value                                                                       |
|---------------------------|-----------------------------------------------|-----------------------------------|-------------------------------------------------------------------------------------|
| `--train_dataset_path`,<br> `-ptrn` | Path to the training dataset            | String                            | `/kaggle/input/aksharantar/aksharantar_sampled/ben/ben_train.csv`                |
| `--test_dataset_path`,<br> `-ptst`  | Path to the testing dataset             | String                            | `/kaggle/input/aksharantar/aksharantar_sampled/ben/ben_test.csv`                 |
| `--epochs`,<br> `-ep`               | Number of epochs for training          | Integer                           | `10`                                                                                |
| `--optimizer`,<br> `-opt`           | Optimizer for training                 | `'adam'`                          | `adam`                                                                              |
| `--batch_size`,<br> `-bs`           | Batch size for training                | Integer                           | `64`                                                                                |
| `--input_embed_size`,<br> `-ies`    | Size of the input embedding            | Integer                           | `16, 32, 64, 256, 512`                                                              |
| `--num_enc_layers`,<br> `-nel`      | Number of layers in the encoder        | Integer                           | `2`                                                                                 |
| `--num_dec_layers`,<br> `-ndl`      | Number of layers in the decoder        | Integer                           | `3`                                                                                 |
| `--hid_layer_size`,<br> `-hls`      | Size of the hidden layer               | Integer                           | `16, 32, 64, 256, 512`                                                              |
| `--cell_type`,<br> `-ct`            | Type of RNN cell for encoder and decoder | `'lstm'`                        | `lstm`                                                                              |
| `--bidirectional`,<br> `-bd`        | Whether to use bidirectional RNN layers  | Boolean                           | `True`                                                                              |
| `--dropout`,<br> `-dp`              | Dropout rate for regularization        | Float                             | `0.2, 0.3`                                                                          |
| `--new_learning_rate`,<br> `-lr`    | Learning rate for the optimizer        | Float                             | `0.001`                                                                             |

