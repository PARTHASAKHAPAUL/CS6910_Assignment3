{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8151136,"sourceType":"datasetVersion","datasetId":4820823}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Importing all the necessary libraries**","metadata":{}},{"cell_type":"code","source":"# # Create Adam optimizer with default parameters\n# optimizer = torch.optim.Adam(model.parameters())\n\n# # Modify learning rate\n# new_learning_rate = 0.001  # Set your desired learning rate\n# for param_group in optimizer.param_groups:\n#     param_group['lr'] = new_learning_rate\n\n# # Modify other parameters\n# # For example, to change weight decay\n# new_weight_decay = 0.01  # Set your desired weight decay value\n# for param_group in optimizer.param_groups:\n#     param_group['weight_decay'] = new_weight_decay\n","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport pandas as pd\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-05-15T19:59:29.934333Z","iopub.execute_input":"2024-05-15T19:59:29.934808Z","iopub.status.idle":"2024-05-15T19:59:34.178992Z","shell.execute_reply.started":"2024-05-15T19:59:29.934776Z","shell.execute_reply":"2024-05-15T19:59:34.177926Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## **Encoder class**","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, rnn_cell='lstm', dropout=0.5, bidirectional=True):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_size)\n        self.dropout = nn.Dropout(dropout)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n        \n        rnn_hidden_size = hidden_size // 2 if bidirectional else hidden_size\n        \n        if rnn_cell.lower() == 'lstm':\n            self.rnn = nn.LSTM(embedding_size, rnn_hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout), bidirectional=bidirectional)\n        elif rnn_cell.lower() == 'gru':\n            self.rnn = nn.GRU(embedding_size, rnn_hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout), bidirectional=bidirectional)\n        else:\n            self.rnn = nn.RNN(embedding_size, rnn_hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout), bidirectional=bidirectional)\n    \n    def forward(self, x):\n        embedded = self.embedding(x)\n        embedded = self.dropout(embedded)\n        outputs, hidden = self.rnn(embedded)\n\n        if self.bidirectional:\n            if isinstance(hidden, tuple):\n                h_n, c_n = hidden\n#                 print('enc h bef dir',h_n.shape)\n#                 print('enc c bef dir',c_n.shape)\n                h_n = torch.cat((h_n[0::2], h_n[1::2]), dim=2)\n                c_n = torch.cat((c_n[0::2], c_n[1::2]), dim=2)\n#                 print('enc h af dir',h_n.shape)\n#                 print('enc c af dir',c_n.shape)\n                hidden = (h_n, c_n)\n            else:\n#                 print('enc hidd bef dir',hidden.shape)\n                hidden = torch.cat((hidden[0::2], hidden[1::2]), dim=2)\n#                 print('after dir enc:',hidden.shape)\n\n        return outputs, hidden","metadata":{"execution":{"iopub.status.busy":"2024-05-15T19:59:43.751963Z","iopub.execute_input":"2024-05-15T19:59:43.752467Z","iopub.status.idle":"2024-05-15T19:59:43.766385Z","shell.execute_reply.started":"2024-05-15T19:59:43.752438Z","shell.execute_reply":"2024-05-15T19:59:43.765484Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## **Attention Module**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_size, rnn_cell):\n        super(Attention, self).__init__()\n        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n        self.v = nn.Parameter(torch.rand(hidden_size))\n        self.rnn_cell = rnn_cell\n    \n    def forward(self, hidden, encoder_outputs):\n        timestep = encoder_outputs.size(1)\n#         print(hidden.shape)\n        if self.rnn_cell == 'lstm':\n            hidden = hidden[-1]  # shape: z[batch_size, hidden_size]\n        else:\n            hidden = hidden\n        h = hidden.unsqueeze(1).repeat(1, timestep, 1)\n#         print('h',h.shape)\n        encoder_outputs = encoder_outputs.permute(0, 1, 2)  # Change to [batch_size, seq_len, hidden_size]\n#         print('encoder_outputs',encoder_outputs.shape)\n        attn_energies = self.score(h, encoder_outputs)\n#         print('attn_energies',attn_energies.shape)\n        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n    \n    def score(self, hidden, encoder_outputs):\n#         print('hidden',hidden.shape)\n#         print('encoder_outputs',encoder_outputs.shape)\n        energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n#         print('energy',energy.shape)\n        energy = energy.permute(0, 2, 1)  # Change to [batch_size, hidden_size, seq_len]\n#         print('energy',energy.shape)\n        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n#         print('v',v.shape)\n        energy = torch.bmm(v, energy)\n#         print('energy',energy.shape)\n        return energy.squeeze(1)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T19:59:45.849581Z","iopub.execute_input":"2024-05-15T19:59:45.850218Z","iopub.status.idle":"2024-05-15T19:59:45.860138Z","shell.execute_reply.started":"2024-05-15T19:59:45.850187Z","shell.execute_reply":"2024-05-15T19:59:45.859032Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## **Decoder class**","metadata":{}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, output_size, embedding_size, hidden_size, num_layers, encoder_num_layers, attention, rnn_cell='lstm', dropout=0.5, bidirectional=True):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(num_embeddings=output_size, embedding_dim=embedding_size)\n        self.dropout = nn.Dropout(dropout)\n        self.output_size = output_size\n        self.hidden_size = hidden_size * encoder_num_layers if bidirectional else hidden_size\n        self.num_layers = num_layers\n        self.attention = attention\n        \n        if rnn_cell.lower() == 'lstm':\n            self.rnn = nn.LSTM(embedding_size + hidden_size * encoder_num_layers, self.hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout))\n        elif rnn_cell.lower() == 'gru':\n            self.rnn = nn.GRU(embedding_size + hidden_size * encoder_num_layers, self.hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout))\n        else:\n            self.rnn = nn.RNN(embedding_size + hidden_size * encoder_num_layers, self.hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout))\n        \n        self.fc = nn.Linear(self.hidden_size, output_size)\n        \n    def forward(self, x, hidden, encoder_outputs):\n        x = x.unsqueeze(1)\n        embedded = self.dropout(self.embedding(x))\n        \n        attn_weights = self.attention(hidden[-1], encoder_outputs)\n        context = attn_weights.bmm(encoder_outputs)\n        rnn_input = torch.cat((embedded, context), 2)\n        \n        output, hidden = self.rnn(rnn_input, hidden)\n        output = self.fc(self.dropout(output.squeeze(1)))\n        \n        return output, hidden, attn_weights\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T19:59:47.627267Z","iopub.execute_input":"2024-05-15T19:59:47.627643Z","iopub.status.idle":"2024-05-15T19:59:47.640055Z","shell.execute_reply.started":"2024-05-15T19:59:47.627615Z","shell.execute_reply":"2024-05-15T19:59:47.639034Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## **Sequence to Sequence model for the above encoder and decoder**","metadata":{}},{"cell_type":"code","source":"class Seq_to_Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq_to_Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teaching_force_ratio=0.5):\n        batch_size = source.size(0)\n        target_len = target.size(1)\n        target_vocab_size = self.decoder.output_size\n        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(source.device)\n#         print(source.shape)#########################\n        encoder_outputs, encoder_hidden = self.encoder(source)\n        \n        if isinstance(encoder_hidden, tuple):\n            h_n, c_n = encoder_hidden\n            if self.encoder.bidirectional:\n                h_n = torch.cat([h_n[i:i+1] for i in range(0, h_n.shape[0], 2)] + [h_n[i:i+1] for i in range(1, h_n.shape[0], 2)], dim=2)\n                c_n = torch.cat([c_n[i:i+1] for i in range(0, c_n.shape[0], 2)] + [c_n[i:i+1] for i in range(1, c_n.shape[0], 2)], dim=2)\n            \n            if h_n.size(0) < self.decoder.num_layers:\n                zero_h = torch.zeros(self.decoder.num_layers - h_n.size(0), batch_size, self.encoder.num_layers * self.encoder.hidden_size, device=h_n.device)\n                zero_c = torch.zeros(self.decoder.num_layers - c_n.size(0), batch_size, self.encoder.num_layers * self.encoder.hidden_size, device=c_n.device)\n                h_n = torch.cat([h_n, zero_h], dim=0)\n                c_n = torch.cat([c_n, zero_c], dim=0)\n\n            encoder_hidden = (h_n[:self.decoder.num_layers], c_n[:self.decoder.num_layers])\n        else:\n            h_n = encoder_hidden\n            if self.encoder.bidirectional:\n                h_n = torch.cat([h_n[i:i+1] for i in range(0, h_n.shape[0], 2)] + [h_n[i:i+1] for i in range(1, h_n.shape[0], 2)], dim=2)\n            \n            if h_n.size(0) < self.decoder.num_layers:\n                zero_h = torch.zeros(self.decoder.num_layers - h_n.size(0), batch_size, self.encoder.num_layers * self.encoder.hidden_size, device=encoder_hidden.device)\n                h_n = torch.cat([h_n, zero_h], dim=0)\n            encoder_hidden = h_n[:self.decoder.num_layers]\n        \n        decoder_input = target[:, 0]\n                    \n        for t in range(1, target_len):\n            decoder_output, encoder_hidden, _ = self.decoder(decoder_input, encoder_hidden, encoder_outputs)\n            outputs[:, t] = decoder_output\n            teacher_force = torch.rand(1) < teaching_force_ratio\n            top1 = decoder_output.argmax(1)\n            decoder_input = target[:, t] if teacher_force else top1\n\n        return outputs\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T19:59:49.606690Z","iopub.execute_input":"2024-05-15T19:59:49.607048Z","iopub.status.idle":"2024-05-15T19:59:49.624042Z","shell.execute_reply.started":"2024-05-15T19:59:49.607022Z","shell.execute_reply":"2024-05-15T19:59:49.623148Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# **Printing the model**","metadata":{}},{"cell_type":"code","source":"INPUT_DIM = 100\nOUTPUT_DIM = 100\nENC_EMB_DIM = 256\nDEC_EMB_DIM = 256\nHID_DIM = 512\nENC_LAYERS = 1\nDEC_LAYERS = 3\nENC_RNN_CELL = 'gru'\nDEC_RNN_CELL = 'gru'\n\nencoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_LAYERS, ENC_RNN_CELL,dropout=0.3, bidirectional = True)\ndecoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_LAYERS, encoder.num_layers, DEC_RNN_CELL, dropout=0.3, bidirectional = True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nmodel = Seq_to_Seq(encoder, decoder).to(device)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T19:59:52.282268Z","iopub.execute_input":"2024-05-15T19:59:52.282904Z","iopub.status.idle":"2024-05-15T19:59:52.700698Z","shell.execute_reply.started":"2024-05-15T19:59:52.282867Z","shell.execute_reply":"2024-05-15T19:59:52.699817Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Using device: cuda\nSeq_to_Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(100, 256)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (rnn): GRU(256, 256, batch_first=True, bidirectional=True)\n  )\n  (decoder): Decoder(\n    (embedding): Embedding(100, 256)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (rnn): LSTM(768, 512, num_layers=3, batch_first=True, dropout=0.3)\n    (fc): Linear(in_features=512, out_features=100, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **A function to create a vocabulary set from the given text**","metadata":{}},{"cell_type":"code","source":"\n# Define a function to create a vocabulary set from a given text\ndef create_vocab(text):\n    # Create a set of unique characters found in the text\n    # Each word in the text is processed to extract its characters\n    vocab = set(char for word in text for char in word)\n    # Add a padding token to the vocabulary\n    vocab.add('<pad>')\n    # Add a start-of-sequence token to the vocabulary\n    vocab.add('<sos>')  # Start of sequence token\n    # Add an end-of-sequence token to the vocabulary\n    vocab.add('<eos>')  # End of sequence token\n    # Return the complete set of vocabulary items\n    return vocab","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-15T19:59:53.601275Z","iopub.execute_input":"2024-05-15T19:59:53.601636Z","iopub.status.idle":"2024-05-15T19:59:53.607736Z","shell.execute_reply.started":"2024-05-15T19:59:53.601610Z","shell.execute_reply":"2024-05-15T19:59:53.606641Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# **A function to load data from a CSV file**","metadata":{}},{"cell_type":"code","source":"# Define a function to load data from a CSV file\ndef load_data(path):\n    # The file has no header and columns are named as 'latin' and 'bangla'\n    df = pd.read_csv(path, header=None, names=['latin', 'bangla'])\n#     df = df.head(10)\n    # Return the columns as two separate Series objects\n    return df['latin'], df['bangla']","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-15T19:59:55.131019Z","iopub.execute_input":"2024-05-15T19:59:55.131608Z","iopub.status.idle":"2024-05-15T19:59:55.136462Z","shell.execute_reply.started":"2024-05-15T19:59:55.131578Z","shell.execute_reply":"2024-05-15T19:59:55.135483Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **Load Latin and bangla training data**","metadata":{}},{"cell_type":"code","source":"# Load Latin and bangla training data from specified path\nlatin_train, bangla_train = load_data('/kaggle/input/aksharantar/aksharantar_sampled/ben/ben_train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-15T19:59:55.649024Z","iopub.execute_input":"2024-05-15T19:59:55.649354Z","iopub.status.idle":"2024-05-15T19:59:55.777191Z","shell.execute_reply.started":"2024-05-15T19:59:55.649329Z","shell.execute_reply":"2024-05-15T19:59:55.776170Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# **Print the loaded Latin and Bangla training data**","metadata":{}},{"cell_type":"code","source":"# Print the loaded Latin training data\nprint(latin_train)\nprint()\n# Print the loaded bangla training data\nprint(bangla_train)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-15T19:59:56.285089Z","iopub.execute_input":"2024-05-15T19:59:56.285431Z","iopub.status.idle":"2024-05-15T19:59:56.294212Z","shell.execute_reply.started":"2024-05-15T19:59:56.285405Z","shell.execute_reply":"2024-05-15T19:59:56.293162Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"0        namdharirao\n1        hindukusher\n2        farajikandi\n3           moubarak\n4             chiung\n            ...     \n51195       silmadar\n51196        jonnote\n51197      handibage\n51198         borpar\n51199     bideshikei\nName: latin, Length: 51200, dtype: object\n\n0            নামধারীরাও\n1           হিন্দুকুশের\n2           ফরাজীকান্দি\n3                মুবারক\n4                চিয়ুং\n              ...      \n51195          সিলমাদার\n51196            জন্যতে\n51197    হ্যান্ডিব্যাগে\n51198             বরপার\n51199         বিদেশীকেই\nName: bangla, Length: 51200, dtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Create two vocabularies from the Latin and Bangla training data**","metadata":{}},{"cell_type":"code","source":"# Create a vocabulary from the Latin training data\nlatin_vocab = create_vocab(latin_train)\n# Create a vocabulary from the bangla training data\nbangla_vocab = create_vocab(bangla_train)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-15T19:59:56.654651Z","iopub.execute_input":"2024-05-15T19:59:56.655490Z","iopub.status.idle":"2024-05-15T19:59:56.750063Z","shell.execute_reply.started":"2024-05-15T19:59:56.655460Z","shell.execute_reply":"2024-05-15T19:59:56.749162Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# **Print the created Latin and Bangla vocabularies**","metadata":{}},{"cell_type":"code","source":"# Print the created Latin vocabulary\nprint(latin_vocab)\nprint()\n# Print the created bangla vocabulary\nprint(bangla_vocab)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-15T19:59:57.233276Z","iopub.execute_input":"2024-05-15T19:59:57.233627Z","iopub.status.idle":"2024-05-15T19:59:57.238399Z","shell.execute_reply.started":"2024-05-15T19:59:57.233600Z","shell.execute_reply":"2024-05-15T19:59:57.237558Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"{'g', 'o', '<eos>', 'a', 'r', 'n', 'q', 'k', 'y', 'c', 'x', 'j', 'l', '<pad>', 'p', 'i', '<sos>', 'e', 'b', 's', 'd', 'u', 'w', 'm', 'v', 'f', 'z', 'h', 't'}\n\n{'অ', 'স', 'গ', 'ঠ', 'া', 'ফ', '<eos>', 'ঁ', 'ঈ', 'ড', 'ক', 'ৃ', 'ব', 'ং', 'র', 'হ', 'এ', 'ত', 'ূ', 'ঊ', '্', '়', 'ল', '২', 'ঔ', 'ই', 'ঝ', 'ভ', 'ম', 'দ', 'আ', 'ৎ', 'ও', 'চ', 'শ', 'ট', 'ণ', 'ো', 'প', 'ন', 'ঞ', 'থ', '<pad>', 'জ', 'ু', 'ৌ', 'ঢ', 'ী', 'ঃ', '<sos>', 'ধ', 'ঋ', 'ে', 'ঐ', 'য', 'ি', 'উ', 'খ', 'ৈ', 'ছ', 'ঙ', 'ষ', 'ঘ'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Map each token in the Latin and Bangla vocabularies to a unique index and then Print the dictionaries mapping (Latin tokens to indices) and (Bangla tokens to indices)**\n","metadata":{}},{"cell_type":"code","source":"# Map each token in the Latin vocabulary to a unique index\nlatin_token_to_index = {token: index for index, token in enumerate(sorted(latin_vocab))}\n# Map each token in the bangla vocabulary to a unique index\nbangla_token_to_index = {token: index for index, token in enumerate(sorted(bangla_vocab))}\n\n# Print the dictionary mapping Latin tokens to indices\nprint(latin_token_to_index)\nprint()\n\n# Print the dictionary mapping bangla tokens to indices\nprint(bangla_token_to_index)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-15T19:59:57.600083Z","iopub.execute_input":"2024-05-15T19:59:57.600426Z","iopub.status.idle":"2024-05-15T19:59:57.606754Z","shell.execute_reply.started":"2024-05-15T19:59:57.600399Z","shell.execute_reply":"2024-05-15T19:59:57.605827Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"{'<eos>': 0, '<pad>': 1, '<sos>': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28}\n\n{'<eos>': 0, '<pad>': 1, '<sos>': 2, 'ঁ': 3, 'ং': 4, 'ঃ': 5, 'অ': 6, 'আ': 7, 'ই': 8, 'ঈ': 9, 'উ': 10, 'ঊ': 11, 'ঋ': 12, 'এ': 13, 'ঐ': 14, 'ও': 15, 'ঔ': 16, 'ক': 17, 'খ': 18, 'গ': 19, 'ঘ': 20, 'ঙ': 21, 'চ': 22, 'ছ': 23, 'জ': 24, 'ঝ': 25, 'ঞ': 26, 'ট': 27, 'ঠ': 28, 'ড': 29, 'ঢ': 30, 'ণ': 31, 'ত': 32, 'থ': 33, 'দ': 34, 'ধ': 35, 'ন': 36, 'প': 37, 'ফ': 38, 'ব': 39, 'ভ': 40, 'ম': 41, 'য': 42, 'র': 43, 'ল': 44, 'শ': 45, 'ষ': 46, 'স': 47, 'হ': 48, '়': 49, 'া': 50, 'ি': 51, 'ী': 52, 'ু': 53, 'ূ': 54, 'ৃ': 55, 'ে': 56, 'ৈ': 57, 'ো': 58, 'ৌ': 59, '্': 60, 'ৎ': 61, '২': 62}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Defining a Dataset class for handling Latin and Bangla word pairs**","metadata":{}},{"cell_type":"code","source":"# Define a Dataset class for handling Latin and Bangla word pairs\nclass AksharantarDataset(Dataset):\n    def __init__(self, latin_words, bangla_words, latin_token_to_index, bangla_token_to_index):\n        # Store the lists of Latin and Bangla words\n        self.latin_words = latin_words\n        self.bangla_words = bangla_words\n        # Store the dictionaries that map characters to indices for both languages\n        self.latin_token_to_index = latin_token_to_index\n        self.bangla_token_to_index = bangla_token_to_index\n\n    def __len__(self):\n        # Return the number of word pairs in the dataset\n        return len(self.latin_words)\n\n    def __getitem__(self, index):\n        # Fetching the Latin and Bangla words at the specified index\n        latin_word = self.latin_words.iloc[index]\n#         print(latin_word)\n        bangla_word = self.bangla_words.iloc[index]\n#         print(bangla_word)\n        # Convert the Latin word into indices using the latin_token_to_index mapping\n        latin_indices = [latin_token_to_index[char] for char in latin_word]\n#         print(latin_indices)\n        # Convert the Bangla word into indices, adding <sos> and <eos> tokens\n        bangla_indices = [bangla_token_to_index['<sos>']] + [bangla_token_to_index[char] for char in bangla_word] + [bangla_token_to_index['<eos>']]\n#         print(bangla_indices)\n        # Return the indices as tensor objects\n        return torch.tensor(latin_indices, dtype=torch.long), torch.tensor(bangla_indices, dtype=torch.long)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-15T19:59:58.122845Z","iopub.execute_input":"2024-05-15T19:59:58.123199Z","iopub.status.idle":"2024-05-15T19:59:58.131714Z","shell.execute_reply.started":"2024-05-15T19:59:58.123172Z","shell.execute_reply":"2024-05-15T19:59:58.130682Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# **Defining a function for padding sequences and packing batches**","metadata":{}},{"cell_type":"code","source":"# Define a function for padding sequences and packing batches\n# packet_fn specifies a function to control how batches are created from the individual data items\ndef packet_fn(batch):\n    # Unzip the batch to separate Latin and Bangla indices\n    latin, bangla = zip(*batch)\n#     print(latin, bangla)\n    # Pad the sequences of Latin indices\n    latin_padded = pad_sequence(latin, batch_first=True, padding_value=latin_token_to_index['<pad>'])\n#     print(latin_padded)\n    # Pad the sequences of Bangla indices\n    bangla_padded = pad_sequence(bangla, batch_first=True, padding_value=bangla_token_to_index['<pad>'])\n#     print(bangla_padded)\n    # Return the padded batches\n    return latin_padded, bangla_padded","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-15T19:59:58.534716Z","iopub.execute_input":"2024-05-15T19:59:58.535132Z","iopub.status.idle":"2024-05-15T19:59:58.541180Z","shell.execute_reply.started":"2024-05-15T19:59:58.535100Z","shell.execute_reply":"2024-05-15T19:59:58.540182Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# **Load training data into the AksharantarDataset and then creating the train_loader by Dataloader function**","metadata":{}},{"cell_type":"code","source":"# Load training data into the AksharantarDataset\ntrain_dataset = AksharantarDataset(latin_train, bangla_train, latin_token_to_index, bangla_token_to_index)\n# Create a DataLoader to batch and shuffle the dataset\n# packet_fn specifies a function to control how batches are created from the individual data items\ntrain_loader = DataLoader(train_dataset, batch_size = 64, collate_fn=packet_fn, shuffle=True)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-15T19:59:58.994229Z","iopub.execute_input":"2024-05-15T19:59:58.995105Z","iopub.status.idle":"2024-05-15T19:59:59.000441Z","shell.execute_reply.started":"2024-05-15T19:59:58.995063Z","shell.execute_reply":"2024-05-15T19:59:58.999476Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# **Print an example from the dataset**","metadata":{}},{"cell_type":"code","source":"# Print an example from the dataset\nprint(train_dataset[4000])\n# for i,j in train_loader:\n#     print(i,'\\n\\n\\n',j)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T19:59:59.678403Z","iopub.execute_input":"2024-05-15T19:59:59.678820Z","iopub.status.idle":"2024-05-15T19:59:59.768216Z","shell.execute_reply.started":"2024-05-15T19:59:59.678791Z","shell.execute_reply":"2024-05-15T19:59:59.767078Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"(tensor([19, 23,  7, 20,  5,  7, 22, 11, 16]), tensor([ 2, 17, 50, 43, 47, 56, 32, 51, 36,  0]))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\n# **A function for calculating word accuracy per batch, ignoring the padding token**","metadata":{}},{"cell_type":"code","source":"# Define a word accuracy function for word-level accuracy\ndef word_accuracy(outputs, targets, ignore_index):\n    # Assuming outputs and targets are batched sequences of token indices\n    # Ignoring <pad> tokens as specified by `ignore_index`\n    correct = 0\n    total = 0\n    for out, tar in zip(outputs, targets):\n        # Ignoring padding in accuracy calculation\n#         print('out bef pad:',out)\n#         print('tar:',tar)\n        out = out[out != ignore_index]\n        tar = tar[tar != ignore_index]\n        ignore_index_eos = 0\n        out = out[out != ignore_index_eos]\n        tar = tar[tar != ignore_index_eos]\n#         print('out aft pad:',out)\n#         print('tar:',tar)\n        if torch.equal(out, tar):\n            correct += 1\n#             print('correct:',correct)\n        total += 1\n#         print('total:',total)\n    return correct / total if total > 0 else 0","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-15T20:00:00.408430Z","iopub.execute_input":"2024-05-15T20:00:00.409289Z","iopub.status.idle":"2024-05-15T20:00:00.415871Z","shell.execute_reply.started":"2024-05-15T20:00:00.409258Z","shell.execute_reply":"2024-05-15T20:00:00.414880Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"\n\n# **Defining the Training function**","metadata":{}},{"cell_type":"code","source":"\ndef train(model, iterator, optimizer, criterion, clip, device, ignore_index):\n    model.train()\n    epoch_loss = 0\n    epoch_acc = 0\n    \n    for source, target in iterator:\n        source = source.to(device)\n        target = target.to(device)\n        \n        optimizer.zero_grad()\n        output = model(source, target)\n        \n        output_dim = output.shape[-1]\n        # Slice to ignore the <sos> token and keep sequence structure\n        output = output[:, 1:, :]\n        target = target[:, 1:]\n        \n        # Flatten all dimensions except for the batch dimension for loss calculation\n        output_flat = output.reshape(-1, output_dim)\n        target_flat = target.reshape(-1)\n        \n#         print('trainnnnnnnn')\n        \n        loss = criterion(output_flat, target_flat)\n        # Calculate word-by-word accuracy\n        acc = word_accuracy(output.argmax(dim=2), target, ignore_index)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc\n    \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n######################","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-15T20:00:01.574560Z","iopub.execute_input":"2024-05-15T20:00:01.574933Z","iopub.status.idle":"2024-05-15T20:00:01.583721Z","shell.execute_reply.started":"2024-05-15T20:00:01.574907Z","shell.execute_reply":"2024-05-15T20:00:01.582763Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# **Defining the Evaluation function**","metadata":{"execution":{"iopub.status.busy":"2024-05-02T03:12:54.052339Z","iopub.execute_input":"2024-05-02T03:12:54.053023Z","iopub.status.idle":"2024-05-02T03:12:54.056815Z","shell.execute_reply.started":"2024-05-02T03:12:54.052993Z","shell.execute_reply":"2024-05-02T03:12:54.055862Z"}}},{"cell_type":"code","source":"def evaluate(model, iterator, criterion, device, ignore_index):\n    model.eval()\n    epoch_loss = 0\n    epoch_acc = 0\n    \n    with torch.no_grad():\n        for source, target in iterator:\n            source = source.to(device)\n            target = target.to(device)\n            \n            output = model(source, target, 0)\n            output_dim = output.shape[-1]\n            output = output[:, 1:, :]\n            target = target[:, 1:]\n            \n            output_flat = output.reshape(-1, output_dim)\n            target_flat = target.reshape(-1)\n#             print('vallllllll')\n            loss = criterion(output_flat, target_flat)\n            acc = word_accuracy(output.argmax(dim=2), target, ignore_index)\n            \n            epoch_loss += loss.item()\n            epoch_acc += acc\n            \n#             break\n    \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\n#######################","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-15T20:00:03.167298Z","iopub.execute_input":"2024-05-15T20:00:03.167667Z","iopub.status.idle":"2024-05-15T20:00:03.175609Z","shell.execute_reply.started":"2024-05-15T20:00:03.167639Z","shell.execute_reply":"2024-05-15T20:00:03.174663Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# **Load validation data into the AksharantarDataset and then creating the valid_loader by Dataloader function**","metadata":{}},{"cell_type":"code","source":"# Load validation data by reading a CSV file\nlatin_valid, bangla_valid = load_data('/kaggle/input/aksharantar/aksharantar_sampled/ben/ben_valid.csv')\n\n# Create a validation dataset using the AksharantarDataset class.\nvalid_dataset = AksharantarDataset(latin_valid, bangla_valid, latin_token_to_index, bangla_token_to_index)\n\n# Create a DataLoader to batch and shuffle the dataset\n# 'collate_fn=packet_fn' specifies a function to control how batches are created from the individual data items.\n# 'shuffle=True' ensures that the data is shuffled at every epoch which helps to reduce model overfitting\nvalid_loader = DataLoader(valid_dataset, batch_size=64, collate_fn=packet_fn, shuffle=True)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-05-15T20:00:04.211181Z","iopub.execute_input":"2024-05-15T20:00:04.211752Z","iopub.status.idle":"2024-05-15T20:00:04.233870Z","shell.execute_reply.started":"2024-05-15T20:00:04.211719Z","shell.execute_reply":"2024-05-15T20:00:04.233081Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# **The training process for specified number of epochs**","metadata":{}},{"cell_type":"code","source":"INPUT_DIM = 100\nOUTPUT_DIM = 100\nENC_EMB_DIM = 256\nDEC_EMB_DIM = 256\nHID_DIM = 512\nENC_LAYERS = 1\nDEC_LAYERS = 1\nENC_RNN_CELL = 'gru'\nDEC_RNN_CELL = 'gru'\n\n# Adjust the model initialization and training process as needed\nencoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_LAYERS, ENC_RNN_CELL, dropout=0.3, bidirectional=True)\nattention = Attention(HID_DIM, ENC_RNN_CELL)\ndecoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_LAYERS, encoder.num_layers, attention, DEC_RNN_CELL, dropout=0.3, bidirectional = True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nmodel = Seq_to_Seq(encoder, decoder).to(device)\nprint(model)\n\nNUM_EPOCHS = 1\nCLIP = 1\noptimizer = torch.optim.Adam(model.parameters())\nignore_index = bangla_token_to_index['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=ignore_index).to(device)\n\nfor epoch in range(NUM_EPOCHS):\n    train_loss, train_accuracy = train(model, train_loader, optimizer, criterion, CLIP, device, ignore_index)\n    val_loss, val_accuracy = evaluate(model, valid_loader, criterion, device, ignore_index)\n    \n    print(f'Epoch: {epoch+1}')\n    print(f'\\tTrain_Loss: {train_loss:.3f}, Train_Accuracy: {train_accuracy*100:.2f}%')\n    print(f'\\tVal_Loss: {val_loss:.3f},  Val_Accuracy: {val_accuracy*100:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T20:00:05.570711Z","iopub.execute_input":"2024-05-15T20:00:05.571082Z","iopub.status.idle":"2024-05-15T20:00:10.128808Z","shell.execute_reply.started":"2024-05-15T20:00:05.571055Z","shell.execute_reply":"2024-05-15T20:00:10.127385Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Using device: cuda\nSeq_to_Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(100, 256)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (rnn): GRU(256, 256, batch_first=True, bidirectional=True)\n  )\n  (decoder): Decoder(\n    (embedding): Embedding(100, 256)\n    (dropout): Dropout(p=0.3, inplace=False)\n    (attention): Attention(\n      (attn): Linear(in_features=1024, out_features=512, bias=True)\n    )\n    (rnn): GRU(768, 512, batch_first=True)\n    (fc): Linear(in_features=512, out_features=100, bias=True)\n  )\n)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39mignore_index)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m---> 27\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, valid_loader, criterion, device, ignore_index)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[19], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip, device, ignore_index)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Calculate word-by-word accuracy\u001b[39;00m\n\u001b[1;32m     26\u001b[0m acc \u001b[38;5;241m=\u001b[39m word_accuracy(output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m), target, ignore_index)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# **Load the Test data into the AksharantarDataset and then creating the test_loader by Dataloader function**","metadata":{}},{"cell_type":"code","source":"# Load the test data from the specified CSV file location\nlatin_test, bangla_test = load_data('/kaggle/input/aksharantar/aksharantar_sampled/ben/ben_test.csv')\n\n# Create test_dataset using the AksharantarDataset class, initializing it with test data\n# and corresponding token-to-index mappings for both Latin and Bangla scripts\ntest_dataset = AksharantarDataset(latin_test, bangla_test, latin_token_to_index, bangla_token_to_index)\n\n# A DataLoader for the test dataset. Here, the batch size is set to 1, indicates\n# that the model will process one item at a time. This is for testing to make\n# detailed predictions per sample without batching effects.\ntest_loader = DataLoader(test_dataset, batch_size=32, collate_fn=packet_fn, shuffle=False)\n# print(test_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:09:40.602548Z","iopub.execute_input":"2024-05-15T18:09:40.603347Z","iopub.status.idle":"2024-05-15T18:09:40.635760Z","shell.execute_reply.started":"2024-05-15T18:09:40.603311Z","shell.execute_reply":"2024-05-15T18:09:40.634769Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"\n\n# **A function to convert an array of indices back into a string, excluding any indices corresponding to special tokens like padding, start, or end of sequence tokens, which should not appear in the final output string**","metadata":{}},{"cell_type":"code","source":"def decode_indices(indices, index_to_token):\n    # Filter out indices for padding, start-of-sequence, and end-of-sequence tokens to ensure only valid character indices are decoded\n    valid_indices = [index for index in indices if index in index_to_token and index not in (bangla_token_to_index['<pad>'], bangla_token_to_index['<sos>'], bangla_token_to_index['<eos>'])]\n    # Convert each index to its corresponding character and join them to form the decoded string\n    return ''.join([index_to_token[index] for index in valid_indices])","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:09:41.393034Z","iopub.execute_input":"2024-05-15T18:09:41.393405Z","iopub.status.idle":"2024-05-15T18:09:41.400650Z","shell.execute_reply.started":"2024-05-15T18:09:41.393357Z","shell.execute_reply":"2024-05-15T18:09:41.399416Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# **Creating the prediction function to generate outputs for all samples in the test_loader**","metadata":{}},{"cell_type":"code","source":"def predict(model, iterator, device):\n    # Set the model to evaluation mode to disable dropout or batch normalization effects during inference\n    model.eval()\n    predictions = []\n    # Disables gradient calculations for performance improvement since they are not needed in inference\n    with torch.no_grad():\n        for source, target in iterator:\n            # Ensure the source and target tensors are on the correct device (GPU or CPU)\n            source = source.to(device)\n            target = target.to(device)\n            # Obtain model output without teacher forcing (i.e., the model relies entirely on its predictions)\n            output = model(source, target, 0)\n            # Get the index with the highest probability from output predictions\n            output = output.argmax(2)\n            # Convert tensors to CPU numpy arrays for easier manipulation and extraction\n            source = source.cpu().numpy()\n            output = output.cpu().numpy()\n            target = target.cpu().numpy()\n            # Store the tuple of source and decoded output predictions\n            predictions.append((source, target, output))\n    # Return all predictions made over the iterator\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:09:42.012413Z","iopub.execute_input":"2024-05-15T18:09:42.012771Z","iopub.status.idle":"2024-05-15T18:09:42.020038Z","shell.execute_reply.started":"2024-05-15T18:09:42.012742Z","shell.execute_reply":"2024-05-15T18:09:42.018975Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# **Creating dictionaries to map indices back to its corresponding characters**","metadata":{}},{"cell_type":"code","source":"# Create dictionaries to map indices back to characters, observing the interpretation of prediction outputs\nlatin_index_to_token = {index: char for char, index in latin_token_to_index.items()}\nbangla_index_to_token = {index: char for char, index in bangla_token_to_index.items()}\n# print(latin_index_to_token)\n# print(bangla_index_to_token)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:09:43.383759Z","iopub.execute_input":"2024-05-15T18:09:43.384123Z","iopub.status.idle":"2024-05-15T18:09:43.389220Z","shell.execute_reply.started":"2024-05-15T18:09:43.384094Z","shell.execute_reply":"2024-05-15T18:09:43.388198Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# **Displaying results: Each input text from the test dataset and its corresponding predicted output text are printed. This helps in visually assessing the accuracy and quality of the transliterations produced by the model**","metadata":{}},{"cell_type":"code","source":"# Taking the prediction function to generate outputs for all samples in the test_loader\ntest_predictions = predict(model, test_loader, device)\n# print(test_predictions[1])\n# Loop through the list of tuples containing source and output indices from the test predictions\nfor source_indices, target_indices, output_indices in test_predictions:\n    # Iterate through each example in the batch. This is necessary as batches may contain multiple examples\n    for i in range(source_indices.shape[0]):\n        # Decode the source indices to their corresponding text using the mapping dictionary for Latin script\n        input_text = decode_indices(source_indices[i], latin_index_to_token)\n        \n        target_text = decode_indices(target_indices[i], bangla_index_to_token)\n\n        # Decode the output indices to their corresponding text using the mapping dictionary for Bangla script\n        predicted_text = decode_indices(output_indices[i], bangla_index_to_token)\n        # Print the original input text and its corresponding predicted transliteration\n        print(f'Input Text: {input_text} -> Actual Text: {target_text} -> Predicted Text: {predicted_text}')","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:09:49.459473Z","iopub.execute_input":"2024-05-15T18:09:49.460106Z","iopub.status.idle":"2024-05-15T18:09:49.711515Z","shell.execute_reply.started":"2024-05-15T18:09:49.460070Z","shell.execute_reply":"2024-05-15T18:09:49.710092Z"},"trusted":true},"execution_count":27,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Taking the prediction function to generate outputs for all samples in the test_loader\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print(test_predictions[1])\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Loop through the list of tuples containing source and output indices from the test predictions\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m source_indices, target_indices, output_indices \u001b[38;5;129;01min\u001b[39;00m test_predictions:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Iterate through each example in the batch. This is necessary as batches may contain multiple examples\u001b[39;00m\n","Cell \u001b[0;32mIn[25], line 12\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, iterator, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Obtain model output without teacher forcing (i.e., the model relies entirely on its predictions)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Get the index with the highest probability from output predictions\u001b[39;00m\n\u001b[1;32m     14\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m2\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[5], line 41\u001b[0m, in \u001b[0;36mSeq_to_Seq.forward\u001b[0;34m(self, source, target, teaching_force_ratio)\u001b[0m\n\u001b[1;32m     38\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m target[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, target_len):\n\u001b[0;32m---> 41\u001b[0m     decoder_output, encoder_hidden, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     outputs[:, t] \u001b[38;5;241m=\u001b[39m decoder_output\n\u001b[1;32m     43\u001b[0m     teacher_force \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m teaching_force_ratio\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 24\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x))\n\u001b[0;32m---> 24\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m context \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mbmm(encoder_outputs)\n\u001b[1;32m     26\u001b[0m rnn_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((embedded, context), \u001b[38;5;241m2\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         encoder_outputs \u001b[38;5;241m=\u001b[39m encoder_outputs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Change to [batch_size, seq_len, hidden_size]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#         print('encoder_outputs',encoder_outputs.shape)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m         attn_energies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#         print('attn_energies',attn_energies.shape)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39msoftmax(attn_energies, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n","Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mAttention.score\u001b[0;34m(self, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden, encoder_outputs):\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#         print('hidden',hidden.shape)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#         print('encoder_outputs',encoder_outputs.shape)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m         energy \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#         print('energy',energy.shape)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m         energy \u001b[38;5;241m=\u001b[39m energy\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Change to [batch_size, hidden_size, seq_len]\u001b[39;00m\n","\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 1 but got size 32 for tensor number 1 in the list."],"ename":"RuntimeError","evalue":"Sizes of tensors must match except in dimension 2. Expected size 1 but got size 32 for tensor number 1 in the list.","output_type":"error"}]},{"cell_type":"code","source":"import wandb\nimport numpy as np\nfrom types import SimpleNamespace\nimport random\n\n# key = input('Enter your API:')\nwandb.login(key='25c2257eaf6c22aa056893db14da4ee2bf0a531a')  #25c2257eaf6c22aa056893db14da4ee2bf0a531a","metadata":{"execution":{"iopub.status.busy":"2024-05-15T20:00:22.011761Z","iopub.execute_input":"2024-05-15T20:00:22.012234Z","iopub.status.idle":"2024-05-15T20:00:25.664362Z","shell.execute_reply.started":"2024-05-15T20:00:22.012207Z","shell.execute_reply":"2024-05-15T20:00:25.663371Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"sweep_config = {\n    'method': 'bayes',\n    'name' : 'sweep attn 1',\n    'metric': {\n        'name': 'Val_Accuracy',\n        'goal': 'maximize'\n    },\n    'parameters': {\n        'input_embed_size': {\n            'values': [16,32,64,256,512]\n        },\n        'num_enc_layers':{\n            'values': [1]\n        },\n        'num_dec_layers':{\n            'values': [1]\n        },\n        'hid_layer_size': {\n            'values': [16,32,64,256,512]\n        },\n        'cell_type': {\n            'values': ['rnn', 'gru', 'lstm']\n        },\n        'bidirectional':{\n            'values': [True, False]\n        },\n        'dropout': {\n            'values': [0.2, 0.3]\n        },\n#       'beam search in decoder with different beam sizes': \n    }\n}\n\nsweep_id = wandb.sweep(sweep = sweep_config, project=\"Deep_Learning_A3\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T20:00:35.095298Z","iopub.execute_input":"2024-05-15T20:00:35.095854Z","iopub.status.idle":"2024-05-15T20:00:36.445772Z","shell.execute_reply.started":"2024-05-15T20:00:35.095824Z","shell.execute_reply":"2024-05-15T20:00:36.444782Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Create sweep with ID: vmcuui5c\nSweep URL: https://wandb.ai/parthasakhapaul/Deep_Learning_A3/sweeps/vmcuui5c\n","output_type":"stream"}]},{"cell_type":"code","source":"# import wandb\n\ndef main():\n    # Initialize a new wandb run\n    with wandb.init() as run:\n        # Construct run name from configuration\n        run_name = \"-embed_size-\"+str(wandb.config.input_embed_size)+\"-layers_enc-\"+str(wandb.config.num_enc_layers)+\"-layers_dec-\"+str(wandb.config.num_dec_layers)+\"-hid_size-\"+str(wandb.config.hid_layer_size)+\"-cell_type-\"+wandb.config.cell_type+\"-bidirectional-\"+str(wandb.config.bidirectional)+\"-dropout-\"+str(wandb.config.dropout)\n        wandb.run.name = run_name\n\n        # Constants defining the dimensions of the input and output character sets\n        INPUT_DIM = 100  # size of the Latin character set\n        OUTPUT_DIM = 100  # size of the Bangla character set\n\n        # Constants defining the dimensions of the embeddings for encoder and decoder\n        ENC_EMB_DIM = wandb.config.input_embed_size  # Encoder embedding dimension\n        DEC_EMB_DIM = wandb.config.input_embed_size  # Decoder embedding dimension\n\n        # Constants defining the dimension of the hidden layers for encoder and decoder\n        HID_DIM = wandb.config.hid_layer_size  # Hidden dimension size\n\n        # Constants defining the number of layers for encoder and decoder\n        ENC_LAYERS = wandb.config.num_enc_layers  # Number of layers in the encoder\n        DEC_LAYERS = wandb.config.num_dec_layers  # Number of layers in the decoder\n        \n\n        # Constants defining the type of RNN cell to use for encoder and decoder\n        ENC_RNN_CELL = wandb.config.cell_type  # RNN cell type for the encoder\n        DEC_RNN_CELL = wandb.config.cell_type  # RNN cell type for the decoder\n        \n#         encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_LAYERS, ENC_RNN_CELL, dropout=0.3, bidirectional=True)\n#         decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_LAYERS, encoder.num_layers, attention, DEC_RNN_CELL, dropout=0.3, bidirectional = True)\n        # Instantiate the encoder with specified configurations\n        encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_LAYERS, ENC_RNN_CELL, dropout = wandb.config.dropout, bidirectional = wandb.config.bidirectional)\n        attention = Attention(HID_DIM, ENC_RNN_CELL)\n        # Instantiate the decoder with specified configurations\n        decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_LAYERS, encoder.num_layers, attention, DEC_RNN_CELL, dropout = wandb.config.dropout, bidirectional = wandb.config.bidirectional)\n\n        # Determine the computing device (CUDA if available, otherwise CPU)\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        # Print the device will be used\n        print(f\"Using device: {device}\")\n\n        # Instantiate the Seq_to_Seq model and move it to the chosen computing device\n        model = Seq_to_Seq(encoder, decoder).to(device)\n        print(model)\n        \n        \n        # Setting the number of epochs the training process should run\n        NUM_EPOCHS = 7\n        # Set the maximum norm of the gradients to 1 to prevent exploding gradients\n        CLIP = 1\n        # Initialize the optimizer, Adam\n        optimizer = torch.optim.Adam(model.parameters())\n        # Padding token index should be ignored in loss calculation\n        ignore_index = bangla_token_to_index['<pad>']\n        # Define the loss function with 'ignore_index' to avoid affecting loss calculation with padding tokens\n        criterion = nn.CrossEntropyLoss(ignore_index=ignore_index).to(device)\n\n        # Start the training process for the defined number of epochs\n        for epoch in range(NUM_EPOCHS):\n            # Doing training on the train dataset and return average loss and accuracy\n            train_loss, train_accuracy = train(model, train_loader, optimizer, criterion, CLIP, device, ignore_index)\n            # Evaluating the model on the validation dataset and return average loss and accuracy\n            val_loss, val_accuracy = evaluate(model, valid_loader, criterion, device, ignore_index)\n\n            # Print the loss and accuracy for each epoch\n            print(f'Epoch: {epoch+1}')\n            print(f'\\tTrain_Loss: {train_loss:.3f}, Train_Accuracy: {train_accuracy*100:.2f}%')\n            print(f'\\tVal_Loss: {val_loss:.3f},  Val_Accuracy: {val_accuracy*100:.2f}%')\n            wandb.log({\"train_accuracy\": train_accuracy * 100, \"training_loss\": train_loss})\n            wandb.log({\"Val_Accuracy\": val_accuracy * 100, \"Val_Loss\": val_loss})\n\n\nwandb.agent(sweep_id, function=main, count=1)\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T20:00:49.035348Z","iopub.execute_input":"2024-05-15T20:00:49.035783Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: yq1ebshq with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: rnn\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \thid_layer_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_embed_size: 16\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_dec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_enc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mparthasakhapaul\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240515_200052-yq1ebshq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/parthasakhapaul/Deep_Learning_A3/runs/yq1ebshq' target=\"_blank\">exalted-sweep-1</a></strong> to <a href='https://wandb.ai/parthasakhapaul/Deep_Learning_A3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/parthasakhapaul/Deep_Learning_A3/sweeps/vmcuui5c' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_Learning_A3/sweeps/vmcuui5c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/parthasakhapaul/Deep_Learning_A3' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_Learning_A3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/parthasakhapaul/Deep_Learning_A3/sweeps/vmcuui5c' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_Learning_A3/sweeps/vmcuui5c</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/parthasakhapaul/Deep_Learning_A3/runs/yq1ebshq' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_Learning_A3/runs/yq1ebshq</a>"},"metadata":{}},{"name":"stdout","text":"Using device: cuda\nSeq_to_Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(100, 16)\n    (dropout): Dropout(p=0.2, inplace=False)\n    (rnn): RNN(16, 256, batch_first=True, bidirectional=True)\n  )\n  (decoder): Decoder(\n    (embedding): Embedding(100, 16)\n    (dropout): Dropout(p=0.2, inplace=False)\n    (attention): Attention(\n      (attn): Linear(in_features=1024, out_features=512, bias=True)\n    )\n    (rnn): RNN(528, 512, batch_first=True)\n    (fc): Linear(in_features=512, out_features=100, bias=True)\n  )\n)\nEpoch: 1\n\tTrain_Loss: 1.814, Train_Accuracy: 2.80%\n\tVal_Loss: 1.551,  Val_Accuracy: 9.45%\nEpoch: 2\n\tTrain_Loss: 1.244, Train_Accuracy: 5.73%\n\tVal_Loss: 1.445,  Val_Accuracy: 12.45%\nEpoch: 3\n\tTrain_Loss: 1.084, Train_Accuracy: 7.55%\n\tVal_Loss: 1.415,  Val_Accuracy: 15.53%\nEpoch: 4\n\tTrain_Loss: 0.978, Train_Accuracy: 9.11%\n\tVal_Loss: 1.351,  Val_Accuracy: 13.55%\nEpoch: 5\n\tTrain_Loss: 0.924, Train_Accuracy: 10.20%\n\tVal_Loss: 1.296,  Val_Accuracy: 13.79%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}