{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b775c84f",
   "metadata": {},
   "source": [
    "# Importing all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd49ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2c919",
   "metadata": {},
   "source": [
    "# Encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff44554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, rnn_cell='lstm', dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Create an embedding layer to convert input indices into dense vectors\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_size)\n",
    "        # Dropout layer for regularizing and preventing overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # RNN configuration parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Construction of RNN layers based on specified cell type\n",
    "        if rnn_cell.lower() == 'lstm':\n",
    "            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout))\n",
    "        elif rnn_cell.lower() == 'gru':\n",
    "            self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout))\n",
    "        else:\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: batch_size, sequence_length\n",
    "        # Embed input indices and apply dropout\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # Forward pass through the RNN\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f1ce40",
   "metadata": {},
   "source": [
    "# Decoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cacac8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Decoder class\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, hidden_size, num_layers, rnn_cell='lstm', dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Create an embedding layer to convert target indices into dense vectors\n",
    "        self.embedding = nn.Embedding(num_embeddings=output_size, embedding_dim=embedding_size)\n",
    "        # Dropout layer for regularizing and preventing overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # RNN configuration parameters\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Conditional construction of RNN layers based on specified cell type\n",
    "        if rnn_cell.lower() == 'lstm':\n",
    "            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout))\n",
    "        elif rnn_cell.lower() == 'gru':\n",
    "            self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout))\n",
    "        else:\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout))\n",
    "        \n",
    "        # Fully connected layer to transform RNN outputs to vocabulary space\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Prepare single step (time step) input for RNN\n",
    "        x = x.unsqueeze(1)  # Change shape to (batch_size, 1)\n",
    "        # Embed input and apply dropout\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # RNN forward pass\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        # Transform RNN output to predictions for each vocabulary token\n",
    "        prediction = self.fc(self.dropout(output.squeeze(1)))\n",
    "        return prediction, hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fdbf25",
   "metadata": {},
   "source": [
    "# Sequence to Sequence model for the above encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96de52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Seq_to_Seq model\n",
    "class Seq_to_Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq_to_Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teaching_force_ratio=0.5):\n",
    "        batch_size = source.size(0)\n",
    "        target_len = target.size(1)\n",
    "        target_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        # Initialize a tensor to hold decoder outputs\n",
    "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(source.device)\n",
    "        \n",
    "        # Encode the source sequence\n",
    "        encoder_hidden = self.encoder(source)\n",
    "        # First token to the decoder is the start token which is the first input to the decoder\n",
    "        decoder_input = target[:, 0]\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            # Decode the current token\n",
    "            decoder_output, encoder_hidden = self.decoder(decoder_input, encoder_hidden)\n",
    "            outputs[:, t] = decoder_output\n",
    "            # Determine whether to use teacher forcing\n",
    "            teacher_force = torch.rand(1) < teaching_force_ratio\n",
    "            top1 = decoder_output.argmax(1)\n",
    "            # If teacher forcing, use the actual next token as the next input; else use the predicted token\n",
    "            decoder_input = target[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb4118",
   "metadata": {},
   "source": [
    "# Printing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a53e437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq_to_Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(256, 64)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (rnn): LSTM(64, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(256, 64)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (rnn): LSTM(64, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Example for dimensions and layers\n",
    "INPUT_DIM = 256  # size of the Latin character set\n",
    "OUTPUT_DIM = 256  # size of the Devanagari character set\n",
    "ENC_EMB_DIM = 64\n",
    "DEC_EMB_DIM = 64\n",
    "HID_DIM = 512\n",
    "ENC_LAYERS = 2\n",
    "DEC_LAYERS = 2\n",
    "ENC_RNN_CELL = 'lstm'\n",
    "DEC_RNN_CELL = 'lstm'\n",
    "\n",
    "# Instantiate encoder and decoder\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_LAYERS, ENC_RNN_CELL)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_LAYERS, DEC_RNN_CELL)\n",
    "\n",
    "# Instantiate the Seq_to_Seq model\n",
    "model = Seq_to_Seq(encoder, decoder)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9e024c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550e665b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb9afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a60c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde851f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
