{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8151136,"sourceType":"datasetVersion","datasetId":4820823}],"dockerImageVersionId":30702,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Importing all the necessary libraries**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport pandas as pd\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-19T11:08:41.361093Z","iopub.execute_input":"2024-04-19T11:08:41.362641Z","iopub.status.idle":"2024-04-19T11:08:47.223494Z","shell.execute_reply.started":"2024-04-19T11:08:41.362584Z","shell.execute_reply":"2024-04-19T11:08:47.222044Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# **Encoder class**","metadata":{}},{"cell_type":"code","source":"# Define a class Encoder, which is a subclass of nn.Module\nclass Encoder(nn.Module):\n    # Constructor with parameters for initialization\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, rnn_cell='lstm', dropout=0.5):\n        # Initialize the base class\n        super(Encoder, self).__init__()\n        # Embedding layer that transforms inputs (word indices) into embeddings of a specified size\n        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_size)\n        # Dropout layer for regularizing and preventing overfitting\n        self.dropout = nn.Dropout(dropout)\n        # Store RNN configuration parameters\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Construction of RNN layers based on specified cell type\n        if rnn_cell.lower() == 'lstm':\n            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout))\n        elif rnn_cell.lower() == 'gru':\n            self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout))\n        else:\n            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout))\n    \n    # Define the forward pass which will be called with input x\n    def forward(self, x):\n        # Apply the embedding layer to the input x, which transforms word indices into dense vectors\n        embedded = self.embedding(x)\n        # Apply dropout to the embeddings\n        embedded = self.dropout(embedded)\n        # Pass the embedded and dropout-applied inputs through the RNN layer\n        # Returns the output and the last hidden state\n        outputs, hidden = self.rnn(embedded)\n        # Return only the hidden state; outputs are not needed in this implementation\n        return hidden","metadata":{"execution":{"iopub.status.busy":"2024-04-19T11:09:47.413095Z","iopub.execute_input":"2024-04-19T11:09:47.413654Z","iopub.status.idle":"2024-04-19T11:09:47.426748Z","shell.execute_reply.started":"2024-04-19T11:09:47.413612Z","shell.execute_reply":"2024-04-19T11:09:47.425559Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **Decoder class**","metadata":{}},{"cell_type":"code","source":"# Define a class Decoder, which is a subclass of nn.Module\nclass Decoder(nn.Module):\n    # Constructor with parameters for initialization\n    def __init__(self, output_size, embedding_size, hidden_size, num_layers, rnn_cell='lstm', dropout=0.5):\n        # Initialize the base class\n        super(Decoder, self).__init__()\n        \n        # Embedding layer that maps indices in the target vocabulary to vectors of a specified size\n        self.embedding = nn.Embedding(num_embeddings=output_size, embedding_dim=embedding_size)\n        \n        # Dropout layer for regularizing and preventing overfitting\n        self.dropout = nn.Dropout(dropout)\n        \n        # Store parameters for configuration of the RNN\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Construction of RNN layers based on specified cell type\n        if rnn_cell.lower() == 'lstm':\n            # LSTM layer\n            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout))\n        elif rnn_cell.lower() == 'gru':\n            # GRU layer\n            self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout))\n        else:\n            # Basic RNN layer \n            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True, dropout=(0 if num_layers == 1 else dropout))\n        \n        # Fully connected layer to transform the output of the RNN into the size of the output vocabulary\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    # Define the forward pass method which will be called with input x and the initial hidden state\n    def forward(self, x, hidden):\n        # Prepare input data for the RNN by adding an extra dimension at index 1 (for batch handling)\n        x = x.unsqueeze(1)  # Change shape from (batch_size) to (batch_size, 1)\n        \n        # Apply the embedding layer to x and then apply dropout\n        embedded = self.dropout(self.embedding(x))\n        \n        # Pass the embedded, dropout-applied input and the previous hidden state into the RNN\n        output, hidden = self.rnn(embedded, hidden)\n        \n        # Squeeze the output from RNN to remove the middle dimension (batch_first=True makes it (batch_size, 1, hidden_size))\n        # Apply dropout again before the final transformation\n        output = self.fc(self.dropout(output.squeeze(1)))\n        \n        # Return the output predictions and the hidden state to be used in the next time step\n        return output, hidden\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T11:09:48.599749Z","iopub.execute_input":"2024-04-19T11:09:48.600268Z","iopub.status.idle":"2024-04-19T11:09:48.615626Z","shell.execute_reply.started":"2024-04-19T11:09:48.600230Z","shell.execute_reply":"2024-04-19T11:09:48.614255Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **Sequence to Sequence model for the above encoder and decoder**","metadata":{}},{"cell_type":"code","source":"# Define the Seq_to_Seq model which is a subclass of nn.Module\nclass Seq_to_Seq(nn.Module):\n    # Constructor with parameters for initialization\n    def __init__(self, encoder, decoder):\n        # Initialize the base class\n        super(Seq_to_Seq, self).__init__()\n        # Assign the encoder instance\n        self.encoder = encoder\n        # Assign the decoder instance\n        self.decoder = decoder\n        \n    # Define the forward pass method that takes source data, target data, and a teaching force ratio\n    def forward(self, source, target, teaching_force_ratio=0.5):\n        # Determine the batch size from the source input\n        batch_size = source.size(0)\n        # Determine the target sequence length from the target input\n        target_len = target.size(1)\n        # Get the target vocabulary size from the decoder\n        target_vocab_size = self.decoder.output_size\n        \n        # Initialize a tensor to store the outputs from the decoder\n        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(source.device)\n        \n        # Encode the source input to get the initial hidden state for the decoder\n        encoder_hidden = self.encoder(source)\n        # The first input to the decoder is typically a start token; here, it's the first target token\n        decoder_input = target[:, 0]\n        \n        # Iterate through each position in the target sequence\n        for t in range(1, target_len):\n            # Generate output and update the hidden state from the decoder\n            decoder_output, encoder_hidden = self.decoder(decoder_input, encoder_hidden)\n            # Store the output of the decoder at the corresponding position in the output tensor\n            outputs[:, t] = decoder_output\n            # Determine whether to use teacher forcing based on a random probability compared to the ratio\n            teacher_force = torch.rand(1) < teaching_force_ratio\n            # Get the highest probability token from the decoder's output\n            top1 = decoder_output.argmax(1)\n            # Depending on teacher forcing, use either the true next token or the predicted token as the next input\n            decoder_input = target[:, t] if teacher_force else top1\n        \n        # Return the tensor holding all the decoder outputs\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2024-04-19T11:09:49.090865Z","iopub.execute_input":"2024-04-19T11:09:49.092391Z","iopub.status.idle":"2024-04-19T11:09:49.104659Z","shell.execute_reply.started":"2024-04-19T11:09:49.092339Z","shell.execute_reply":"2024-04-19T11:09:49.103117Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# **Printing the model**","metadata":{}},{"cell_type":"code","source":"# Constants defining the dimensions of the input and output character sets\nINPUT_DIM = 256  # size of the Latin character set\nOUTPUT_DIM = 256  # size of the Bangla character set\n\n# Constants defining the dimensions of the embeddings for encoder and decoder\nENC_EMB_DIM = 64  # Encoder embedding dimension\nDEC_EMB_DIM = 64  # Decoder embedding dimension\n\n# Constants defining the dimension of the hidden layers for encoder and decoder\nHID_DIM = 512  # Hidden dimension size\n\n# Constants defining the number of layers for encoder and decoder\nENC_LAYERS = 2  # Number of layers in the encoder\nDEC_LAYERS = 2  # Number of layers in the decoder\n\n# Constants defining the type of RNN cell to use for encoder and decoder\nENC_RNN_CELL = 'lstm'  # RNN cell type for the encoder\nDEC_RNN_CELL = 'lstm'  # RNN cell type for the decoder\n\n# Instantiate the encoder with specified configurations\nencoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_LAYERS, ENC_RNN_CELL)\n# Instantiate the decoder with specified configurations\ndecoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_LAYERS, DEC_RNN_CELL)\n\n# Determine the computing device (CUDA if available, otherwise CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Print the device will be used\nprint(f\"Using device: {device}\")\n\n# Instantiate the Seq_to_Seq model and move it to the chosen computing device\nmodel = Seq_to_Seq(encoder, decoder).to(device)\n# Print the model architecture\nprint(model)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-19T11:09:49.526642Z","iopub.execute_input":"2024-04-19T11:09:49.527139Z","iopub.status.idle":"2024-04-19T11:09:49.670299Z","shell.execute_reply.started":"2024-04-19T11:09:49.527104Z","shell.execute_reply":"2024-04-19T11:09:49.668931Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Using device: cpu\nSeq_to_Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(256, 64)\n    (dropout): Dropout(p=0.5, inplace=False)\n    (rnn): LSTM(64, 512, num_layers=2, batch_first=True, dropout=0.5)\n  )\n  (decoder): Decoder(\n    (embedding): Embedding(256, 64)\n    (dropout): Dropout(p=0.5, inplace=False)\n    (rnn): LSTM(64, 512, num_layers=2, batch_first=True, dropout=0.5)\n    (fc): Linear(in_features=512, out_features=256, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **A function to create a vocabulary set from the given text**","metadata":{}},{"cell_type":"code","source":"# Define a function to create a vocabulary set from a given text\ndef create_vocab(text):\n    # Create a set of unique characters found in the text\n    # Each word in the text is processed to extract its characters\n    vocab = set(char for word in text for char in word)\n    # Add a padding token to the vocabulary\n    vocab.add('<pad>')\n    # Add a start-of-sequence token to the vocabulary\n    vocab.add('<sos>')  # Start of sequence token\n    # Add an end-of-sequence token to the vocabulary\n    vocab.add('<eos>')  # End of sequence token\n    # Return the complete set of vocabulary items\n    return vocab","metadata":{"execution":{"iopub.status.busy":"2024-04-19T11:09:51.288239Z","iopub.execute_input":"2024-04-19T11:09:51.288751Z","iopub.status.idle":"2024-04-19T11:09:51.296852Z","shell.execute_reply.started":"2024-04-19T11:09:51.288709Z","shell.execute_reply":"2024-04-19T11:09:51.295436Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# **A function to load data from a CSV file**","metadata":{}},{"cell_type":"code","source":"# Define a function to load data from a CSV file\ndef load_data(path):\n    # The file has no header and columns are named as 'latin' and 'bangla'\n    df = pd.read_csv(path, header=None, names=['latin', 'bangla'])\n    # Return the columns as two separate Series objects\n    return df['latin'], df['bangla']","metadata":{"execution":{"iopub.status.busy":"2024-04-19T11:09:53.081041Z","iopub.execute_input":"2024-04-19T11:09:53.081589Z","iopub.status.idle":"2024-04-19T11:09:53.088556Z","shell.execute_reply.started":"2024-04-19T11:09:53.081549Z","shell.execute_reply":"2024-04-19T11:09:53.087083Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **Load Latin and bangla training data**","metadata":{}},{"cell_type":"code","source":"# Load Latin and bangla training data from specified path\nlatin_train, bangla_train = load_data('/kaggle/input/aksharantar/aksharantar_sampled/ben/ben_train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-19T11:09:53.791866Z","iopub.execute_input":"2024-04-19T11:09:53.792420Z","iopub.status.idle":"2024-04-19T11:09:53.951101Z","shell.execute_reply.started":"2024-04-19T11:09:53.792379Z","shell.execute_reply":"2024-04-19T11:09:53.949565Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# **Print the loaded Latin and Bangla training data**","metadata":{}},{"cell_type":"code","source":"# Print the loaded Latin training data\nprint(latin_train)\nprint()\n# Print the loaded bangla training data\nprint(bangla_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T11:09:55.390011Z","iopub.execute_input":"2024-04-19T11:09:55.390589Z","iopub.status.idle":"2024-04-19T11:09:55.403789Z","shell.execute_reply.started":"2024-04-19T11:09:55.390551Z","shell.execute_reply":"2024-04-19T11:09:55.402071Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"0        namdharirao\n1        hindukusher\n2        farajikandi\n3           moubarak\n4             chiung\n            ...     \n51195       silmadar\n51196        jonnote\n51197      handibage\n51198         borpar\n51199     bideshikei\nName: latin, Length: 51200, dtype: object\n\n0            নামধারীরাও\n1           হিন্দুকুশের\n2           ফরাজীকান্দি\n3                মুবারক\n4                চিয়ুং\n              ...      \n51195          সিলমাদার\n51196            জন্যতে\n51197    হ্যান্ডিব্যাগে\n51198             বরপার\n51199         বিদেশীকেই\nName: bangla, Length: 51200, dtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Create two vocabularies from the Latin and Bangla training data**","metadata":{}},{"cell_type":"code","source":"# Create a vocabulary from the Latin training data\nlatin_vocab = create_vocab(latin_train)\n# Create a vocabulary from the bangla training data\nbangla_vocab = create_vocab(bangla_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T11:09:56.638735Z","iopub.execute_input":"2024-04-19T11:09:56.639240Z","iopub.status.idle":"2024-04-19T11:09:56.753861Z","shell.execute_reply.started":"2024-04-19T11:09:56.639206Z","shell.execute_reply":"2024-04-19T11:09:56.752807Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# **Print the created Latin and Bangla vocabularies**","metadata":{}},{"cell_type":"code","source":"# Print the created Latin vocabulary\nprint(latin_vocab)\nprint()\n# Print the created bangla vocabulary\nprint(bangla_vocab)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T11:09:58.664951Z","iopub.execute_input":"2024-04-19T11:09:58.665440Z","iopub.status.idle":"2024-04-19T11:09:58.671979Z","shell.execute_reply.started":"2024-04-19T11:09:58.665406Z","shell.execute_reply":"2024-04-19T11:09:58.670898Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"{'j', 'y', 'a', 'w', 'p', '<eos>', 's', 'h', 'r', 'v', 'd', 'k', 'z', 'f', 'u', 'i', 'c', 'e', '<pad>', 'g', 'l', 'm', 'q', 'b', 'o', 'n', 't', '<sos>', 'x'}\n\n{'ো', 'ণ', 'খ', 'ঃ', 'ে', 'ঢ', 'ী', 'ষ', 'ঝ', 'ঁ', 'র', 'ধ', '<eos>', 'আ', 'প', '২', 'ও', 'ভ', 'ফ', 'ৎ', 'থ', 'ঠ', 'জ', 'অ', 'ং', 'ড', '়', 'ঈ', 'ঞ', 'ু', 'ঊ', 'ক', 'গ', '্', 'ূ', 'ৈ', 'ি', 'ঙ', 'ত', 'উ', 'ঔ', 'শ', 'ৃ', '<pad>', 'ম', 'হ', 'চ', 'ঐ', 'ই', 'দ', 'ব', 'এ', 'ল', 'স', 'ৌ', 'ঘ', 'ঋ', 'ছ', '<sos>', 'য', 'া', 'ট', 'ন'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Map each token in the Latin and Bangla vocabularies to a unique index and then Print the dictionaries mapping (Latin tokens to indices) and (Bangla tokens to indices)**\n","metadata":{}},{"cell_type":"code","source":"# Map each token in the Latin vocabulary to a unique index\nlatin_token_to_index = {token: idx for idx, token in enumerate(sorted(latin_vocab))}\n# Map each token in the bangla vocabulary to a unique index\nbangla_token_to_index = {token: idx for idx, token in enumerate(sorted(bangla_vocab))}\n\n# Print the dictionary mapping Latin tokens to indices\nprint(latin_token_to_index)\nprint()\n\n# Print the dictionary mapping bangla tokens to indices\nprint(bangla_token_to_index)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T11:09:59.780384Z","iopub.execute_input":"2024-04-19T11:09:59.780895Z","iopub.status.idle":"2024-04-19T11:09:59.788631Z","shell.execute_reply.started":"2024-04-19T11:09:59.780855Z","shell.execute_reply":"2024-04-19T11:09:59.787588Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"{'<eos>': 0, '<pad>': 1, '<sos>': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28}\n\n{'<eos>': 0, '<pad>': 1, '<sos>': 2, 'ঁ': 3, 'ং': 4, 'ঃ': 5, 'অ': 6, 'আ': 7, 'ই': 8, 'ঈ': 9, 'উ': 10, 'ঊ': 11, 'ঋ': 12, 'এ': 13, 'ঐ': 14, 'ও': 15, 'ঔ': 16, 'ক': 17, 'খ': 18, 'গ': 19, 'ঘ': 20, 'ঙ': 21, 'চ': 22, 'ছ': 23, 'জ': 24, 'ঝ': 25, 'ঞ': 26, 'ট': 27, 'ঠ': 28, 'ড': 29, 'ঢ': 30, 'ণ': 31, 'ত': 32, 'থ': 33, 'দ': 34, 'ধ': 35, 'ন': 36, 'প': 37, 'ফ': 38, 'ব': 39, 'ভ': 40, 'ম': 41, 'য': 42, 'র': 43, 'ল': 44, 'শ': 45, 'ষ': 46, 'স': 47, 'হ': 48, '়': 49, 'া': 50, 'ি': 51, 'ী': 52, 'ু': 53, 'ূ': 54, 'ৃ': 55, 'ে': 56, 'ৈ': 57, 'ো': 58, 'ৌ': 59, '্': 60, 'ৎ': 61, '২': 62}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Defining a Dataset class for handling Latin and Bangla word pairs**","metadata":{}},{"cell_type":"code","source":"# Define a Dataset class for handling Latin and Bangla word pairs\nclass AksharantarDataset(Dataset):\n    def __init__(self, latin_words, bangla_words, latin_token_to_index, bangla_token_to_index):\n        # Store the lists of Latin and Bangla words\n        self.latin_words = latin_words\n        self.bangla_words = bangla_words\n        # Store the dictionaries that map characters to indices for both languages\n        self.latin_token_to_index = latin_token_to_index\n        self.bangla_token_to_index = bangla_token_to_index\n\n    def __len__(self):\n        # Return the number of word pairs in the dataset\n        return len(self.latin_words)\n\n    def __getitem__(self, idx):\n        # Fetching the Latin and Bangla words at the specified index\n        latin_word = self.latin_words.iloc[idx]\n#         print(latin_word)\n        bangla_word = self.bangla_words.iloc[idx]\n#         print(bangla_word)\n        # Convert the Latin word into indices using the latin_token_to_index mapping\n        latin_indices = [latin_token_to_index[char] for char in latin_word]\n#         print(latin_indices)\n        # Convert the Bangla word into indices, adding <sos> and <eos> tokens\n        bangla_indices = [bangla_token_to_index['<sos>']] + [bangla_token_to_index[char] for char in bangla_word] + [bangla_token_to_index['<eos>']]\n#         print(bangla_indices)\n        # Return the indices as tensor objects\n        return torch.tensor(latin_indices, dtype=torch.long), torch.tensor(bangla_indices, dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T11:12:31.159227Z","iopub.execute_input":"2024-04-19T11:12:31.159734Z","iopub.status.idle":"2024-04-19T11:12:31.171132Z","shell.execute_reply.started":"2024-04-19T11:12:31.159699Z","shell.execute_reply":"2024-04-19T11:12:31.169460Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# **Defining a function for padding sequences and packing batches**","metadata":{}},{"cell_type":"code","source":"# Define a function for padding sequences and packing batches\ndef packet_fn(batch):\n    # Unzip the batch to separate Latin and Bangla indices\n    latin, bangla = zip(*batch)\n#     print(latin, bangla)\n    # Pad the sequences of Latin indices\n    latin_padded = pad_sequence(latin, batch_first=True, padding_value=latin_token_to_index['<pad>'])\n#     print(latin_padded)\n    # Pad the sequences of Bangla indices\n    bangla_padded = pad_sequence(bangla, batch_first=True, padding_value=bangla_token_to_index['<pad>'])\n#     print(bangla_padded)\n    # Return the padded batches\n    return latin_padded, bangla_padded","metadata":{"execution":{"iopub.status.busy":"2024-04-19T11:12:31.813588Z","iopub.execute_input":"2024-04-19T11:12:31.814121Z","iopub.status.idle":"2024-04-19T11:12:31.822782Z","shell.execute_reply.started":"2024-04-19T11:12:31.814078Z","shell.execute_reply":"2024-04-19T11:12:31.821331Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# **Load training data into the AksharantarDataset and then creating the train_loader by Dataloader function**","metadata":{}},{"cell_type":"code","source":"# Load training data into the AksharantarDataset\ntrain_dataset = AksharantarDataset(latin_train, bangla_train, latin_token_to_index, bangla_token_to_index)\n# Create a DataLoader to batch and shuffle the dataset\ntrain_loader = DataLoader(train_dataset, batch_size=32, collate_fn=packet_fn, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T11:12:32.323927Z","iopub.execute_input":"2024-04-19T11:12:32.324496Z","iopub.status.idle":"2024-04-19T11:12:32.331950Z","shell.execute_reply.started":"2024-04-19T11:12:32.324441Z","shell.execute_reply":"2024-04-19T11:12:32.330282Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# **Print an example from the dataset**","metadata":{}},{"cell_type":"code","source":"# Print an example from the dataset\nprint(train_dataset[4897])\n# for i,j in train_loader:\n#     print(i,'\\n\\n\\n',j)","metadata":{"execution":{"iopub.status.busy":"2024-04-19T11:12:32.735656Z","iopub.execute_input":"2024-04-19T11:12:32.736366Z","iopub.status.idle":"2024-04-19T11:12:32.748012Z","shell.execute_reply.started":"2024-04-19T11:12:32.736310Z","shell.execute_reply":"2024-04-19T11:12:32.746462Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"(tensor([13, 17, 22,  3, 15, 21,  3, 20]), tensor([ 2, 17, 58, 27, 41, 47, 43,  0]))\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}